{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br />\n",
    "A linear mixed model in model organism association mapping is typically expressed as <br />\n",
    "$y=xβ+Zu+e$ <br />\n",
    "where y is an n × 1 vector of observed phenotypes, and x is an n × q matrix of fixed effects including mean, SNPs, and other confounding variables. β is a q × 1 vector representing coefficients of the fixed effects. Z is an n × t incidence matrix mapping each observed phenotype to one of t inbred strains. And in general Z is None, and will be ignored at some positons in the fellowing document. u is the random effect of the mixed model with $Var(u) = σ_g^2K$, where K is the t × t kinship matrix, and e is an n × n matrix of residual effect such that $Var(e) = σ_e^2$. The overall phenotypic variance–covariance matrix can be represented as\n",
    "$V=σ_g^2K+σ_e^2I$ <br />\n",
    "<i>\n",
    "This part, in italic font, explains likelihood distribition and the symbols have no relationship with context.<br />\n",
    "For liner model y = xβ. The distribution of $y~N(Xβ, σ^2I)$\n",
    "So the likelihood $L(y;β,σ^2)=\\frac{1}{(2πσ^2)^{\\frac{n}{2}}}e^{-\\frac{(y-xβ)′(y-xβ)}{2σ^2}}$<br />\n",
    "</i>\n",
    "\n",
    "<br />\n",
    "For the linear mixed model: $y~N(xβ, σ_g^2K+σ_e^2I)=N(xβ,σ_g^2H)$<br />\n",
    "$H = σ_g^{-2}V = K+δI$  (there is a typo in the emma paper)<br />\n",
    "$δ = \\frac{σ_e^2}{σ_g^2}$  <br />\n",
    "$likelihood(y;β,σ_g,δ)=\\frac{1}{(2πσ_g^2H)^{\\frac{n}{2}}}e^{-\\frac{(y-xβ)′H^{-1}(y-xβ)}{σ_g^2}}$\n",
    "<br />\n",
    "ML (maximum likelihood):<br />\n",
    "$l_F(y;β,σ_g,δ) = \\frac{1}{2} (-n log(2πσ_g^2) - log|H| - \\frac{1}{σ_g^2}(y-xβ)′H^{-1}(y-xβ))$ <br />\n",
    "REML:\n",
    "$l_R(y;σ_g,δ) = l_F(y;\\hat{β},σ_g,δ) + \\frac{1}{2} (q log(2πσ_g^2) + log|x′x| - log|x′H^{-1}x|)$  <br />\n",
    "Gradient of the LMM log likelihood w.r.t. β <br />\n",
    "$∇_βl_R(y;σ_g,δ)=\\frac{d-\\frac{1}{2σ_g^2}(y-xβ)^T(K+Iδ)^{-1}(y-xβ)}{dβ}=\\frac{1}{σ_g^2}(-x^T(k+Iδ)^{-1}y+x^T(k+Iδ)^{-1}x)$<br />\n",
    "set gradient to zero:<br />\n",
    "$x^TH^{-1}xβ=x^TH^{-1}y$<br />\n",
    "β could be estimated as $\\hat{β}=(x′H^{-1}x)^{-1}x′H^{-1}y$<br />\n",
    "\n",
    "Note that this solution is analogous to the ML solution of the linear\n",
    "regression<br />\n",
    "$(x^tx)^{-1}x^Ty$\n",
    "\n",
    "<br />\n",
    "\n",
    "<br />\n",
    "For ML: $\\hat{σ}_g^2 = \\frac{R}{n}$<br />\n",
    "Then: $l_F(y;\\hat{β},\\hat{σ_g},\\hat{δ})=\\frac{1}{2}(-nlog\\frac{2πR}{n}-log|H|-n)$ <br />\n",
    "\n",
    "\n",
    "For REML: $\\hat{σ}_g^2 = \\frac{R}{n-q}$<br />\n",
    "$R = (y-xβ)′H^{-1}(y-xβ)$<br />\n",
    "$H = K + δI = U_F( ξ_i+δ,...,ξ_n+δ)U_F′  \\\\   K=U_FξU_F′$ with eigen decomposition<br />\n",
    "so $log|H|=\\sum^n_{i=1}log(ξ_i+δ)$<br />\n",
    "And $R=(y-xβ)′H^{-1}(y-xβ)=y′(I-x(x′H^{-1}x)^{-1}x′H^{-1})′H^{-1}(I-x(x′H^{-1}x)^{-1}x′H^{-1})y=y′P′H^{-1}Py$\n",
    "P is defined as $P=I-x(x′H^{-1}x)^{-1}x′H^{-1}$<br />\n",
    "$S = I − X(X′X)^{−1}X′ $<br />\n",
    "$SHS = S(K + δI)S$   do eigen decomposition<br />\n",
    "And $(SHS)(P′H^{-1}P)(SHS)=SHS$<br />\n",
    "$(P′H^{-1}P)(SHS)(P′H^{-1}P)=(P′H^{-1}P)$<br />\n",
    "$PS=P$ (there is a typo in the emma paper)<br />\n",
    "<br />and $SP=S$<br />\n",
    "$SHS = [U_R, W_R]diag(λ_1+δ,...,λ_{n-q}+δ,0,...,0)[U_R, W_R]′ \\\\   \n",
    "     = U_Rdiag(λ_1+δ,...,λ_{n-q}δ,0,...,0)U_R′$<br />\n",
    "\n",
    "$U_R$ is an n × (n − q) eigenvector matrix corresponding to the nonzero eigenvalues. $W_R$ is an n × q eigenvector matrix corresponding to zero eigenvalues. <br />\n",
    "Here the eigen decomposition of H and SHS do not dependent on any unknow parameter and could be done directly <br />\n",
    "\n",
    "so $P′H^{-1}P=(SHS)^+=U_Rdiag((λ_s+δ)^{-1})U_R′$ here $(.)^+$ denotes the pseudo-inverse of a matrix\n",
    "Let $U_R′y = [η_1, η_2,...,η_{n−q}]′$ <br />\n",
    "and $R=y′P′H^{-1}Py=(U_R′y)′diag((λ_s+δ)^{-1})(U_R′y)=\\sum_{s=1}^{n-q}\\frac{η_s^2}{λ_s+δ}$<br />\n",
    "\n",
    "Then for ML: $l_F(y;\\hat{β},\\hat{σ_g},\\hat{δ})=\\frac{1}{2}nlog\\frac{n}{2π}-n-nlog\\sum_{s=1}^{n-q}\\frac{η_s^2}{λ_s+δ}-\\sum_{i=1}^{n}log(ξ_i+δ)$<br />\n",
    "\n",
    "$(SHS)(SHS)^+ = (SHS)(P′H^{-1}P)=SHP′H^{-1}P=SP=S$<br />\n",
    "On the other hand<br />\n",
    "$(SHS)(SHS)^+=(U_Rdiag(λ_s+δ)U_R′)(U_Rdiag((λ_s+δ)^{-1})U_R′)=U_RU_R′$<br />\n",
    "so $U_RU_R′=S=I$<br />\n",
    "taking account $\\hat{σ}_g^2 = \\frac{R}{n-q}$<br />\n",
    "Then for REML: $l_R(y;\\hat{σ_g},\\hat{δ})=\\frac{1}{2}(n-q)log\\frac{n-q}{2π}-(n-q)-(n-q)log\\sum_{s=1}^{n-q}\\frac{η_s^2}{λ_s+δ}-\\sum_{s=1}^{n-q}log(λ_s+δ)$<br />\n",
    "<br />\n",
    "The derivatives of these functions:<br />\n",
    "ML: $f_F′=\\frac{n}{2}\\frac{\\sum_s{\\frac{η_s^2}{(λ_s+δ)^2}}}{\\frac{η_s^2}{λ_s+δ}}-\\frac{1}{2}\\sum_i{\\frac{1}{ξ_i+δ}}$<br />\n",
    "REML: $f_F′=\\frac{n-q}{2}\\frac{\\sum_s{\\frac{η_s^2}{(λ_s+δ)^2}}}{\\frac{η_s^2}{λ_s+δ}}-\\frac{1}{2}\\sum_i{\\frac{1}{ξ_i+δ}}$<br />\n",
    "<br />\n",
    "if we could find B  such that<br />\n",
    "$BB′ = H = \\frac{V}{σ_g^2}=K+δI$<br />\n",
    "we can substitete $y^*=B^{-1}y$, $x^*=B^{-1}x$ and $ϵ^*=B^{-1}(Zu+ϵ)$ (now $ϵ^*$ includes both random effects and errors ) to get<br />\n",
    "$y^*=x^*β+ϵ^*$<br />\n",
    "$Var(ϵ^*)=Var(B^{-1}(Zu+ϵ))=B^{-1}V(B^{-1})′=σ_g^2B^{-1}H(B^{-1})′=σ_g^2B^{-1}BB′(B^{-1})′=σ_g^2I$<br />\n",
    "The value of the residual sum of squares (RSS) from solving the transformed equation $y^* = X^*\\beta + \\epsilon^*$ is the Mahalanobis RSS for the original equation $y = x\\beta + Zu + \\epsilon$.<br />\n",
    "\n",
    "Taking advantage of the eigen decomposition of H performed in the EMMA algorithm, the computation of a valid B^{-1} can be simplified to<br />\n",
    "$B^{-1} = diag(1/\\sqrt{\\xi_1 + \\delta}, ..., 1/\\sqrt{\\xi_n + \\delta}) U_F'$<br />\n",
    "$B^{-1}$ is H_sqrt_inv in the code<br />\n",
    "<br />\n",
    "\n",
    "<br />\n",
    "β could be estimate by solving $y^*=x^*β+ϵ^*$ <br />\n",
    "\n",
    "The F test here is performed on $y^*$ and $x^*$ <br />\n",
    "In the code h0_rss is the Residual sum of squares without the effecte of fixed variable under testing (H0)<br />\n",
    "mahalanobis_rss is the Residual sum of squares with the effecte of fixed variable under testing (H1)<br />\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "If here x is the intercept and all the significant genotypic variants, after β being calculated <br />\n",
    "Then residuals = y - xβ <br />\n",
    "Variance explained by significant genotypic variants is ( var(y)-residuals.T * residuals ) / var(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
